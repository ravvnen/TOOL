@startuml Sequence_AgentQuery
title Sequence Diagram: Chat Flow (User → Agent.UI → TOOL API → Ollama)

skinparam backgroundColor #FEFEFE
skinparam sequenceMessageAlign center

actor "User" as USER
participant "Agent.UI\n(React)" as UI
participant "TOOL API\n(.NET)" as API
participant "Memory\nCompiler" as COMPILER
database "SQLite\n(FTS5)" as DB
participant "Ollama\n(localhost)" as LLM

== User Types Question in Chat ==

USER -> UI: Types "What is the API rate limit?"
activate UI

UI -> UI: handleSendMessage()

== Step 1: Compile Memory from TOOL API ==

UI -> API: POST /api/v1/compile-memory\n{prompt: "...", topK: 6, ns: "..."}
activate API

API -> COMPILER: SelectRelevantAsync(ns, prompt, topK)
activate COMPILER

COMPILER -> DB: FTS5 MATCH query\nSELECT FROM im_items_current\nJOIN im_fts ...
activate DB
DB --> COMPILER: [{item_id, title, content, version}]
deactivate DB

COMPILER -> COMPILER: BuildMemoryJson()\nFormat with rule IDs

COMPILER --> API: CompiledMemory\n{ns, rules: [...], generatedAt}
deactivate COMPILER

API --> UI: 200 OK\n{ns, rules: [{id, title, content, version}]}
deactivate API

== Step 2: Call Ollama with Compiled Memory ==

UI -> UI: Build messages array:\n- system: "You are a coding assistant..."\n- system: INSTRUCTIONS_MEMORY_JSON\n- user: prompt

UI -> LLM: POST /api/chat\n{model: "llama3", messages: [...]}
activate LLM

LLM -> LLM: Generate response\nusing injected rules

LLM --> UI: {message: {content: "The API rate limit is\n100 req/min [im:api.rate_limit@v3]"}}
deactivate LLM

== Step 3: Display Response ==

UI -> UI: Add assistant message\nwith memory context

UI --> USER: Shows response with\nrule citation [im:api.rate_limit@v3]
deactivate UI

note over USER
  User can click rule citation
  to see provenance via
  GET /api/v1/why?id=...
end note

note over UI, LLM
  Key insight: TOOL API never calls LLM directly.
  Agent.UI orchestrates both calls.
  This keeps TOOL focused on memory management.
end note

@enduml
